{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satan_baba/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, re, random\n",
    "import sys, argparse, codecs\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data(text):\n",
    "    '''Re-encodes text so that it can be printed to stdout \n",
    "       without raising a UnicodeEncodeError.\n",
    "       Incompatible characters are simply dropped before printing.\n",
    "\n",
    "       Args:\n",
    "       - text: (str) The text to be printed'''\n",
    "\n",
    "    print(text.encode(sys.stdout.encoding, errors='replace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, encoding='utf-8'):\n",
    "    '''Appends all text files in data_dir into a single string and returns it.\n",
    "       All files are assumed to be utf-8 encoded, and of type '.txt'.\n",
    "\n",
    "       Args:\n",
    "       - data_dir: (str) The directory to text files for training.\n",
    "       - encoding: (str) The type of encoding to use when decoding each file.\n",
    "\n",
    "       Returns:\n",
    "       - text_data: (str) Appended files as a single string.'''\n",
    "\n",
    "    print(\"Loading data from %s\" % os.path.abspath(data_dir))\n",
    "    # Initialise text string\n",
    "    text_data = ''\n",
    "    # select .txt files from data_dir\n",
    "    for filename in filter(lambda s: s.endswith(\".txt\"), os.listdir(data_dir)):\n",
    "        # open file with default encoding\n",
    "        print(\"loading file: %s\" % filename)\n",
    "        filepath = os.path.abspath(os.path.join(data_dir, filename))\n",
    "        with open(filepath,'r', encoding = encoding) as f:\n",
    "            text_data += f.read() + \"\\n\"\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text_data, seq_length):\n",
    "    '''Preprocesses text_data for RNN model.\n",
    "\n",
    "       Args:\n",
    "       - text: (str) text file to be processed.\n",
    "       - seq_length: (int) length of character sequences to be considered \n",
    "                     in the training set.\n",
    "\n",
    "       Returns:\n",
    "       - char_to_int: (dict) Maps characters in the character set to ints.\n",
    "       - int_to_char: (dict) Maps ints to characters in the character set.\n",
    "       - n_chars: (int) The number of characters in the text.\n",
    "       - n_vocab: (int) The number of unique characters in the text.'''\n",
    "\n",
    "    # create mapping of unique chars to integers, and a reverse mapping\n",
    "    chars = sorted(set(text_data))\n",
    "    char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "    int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "    # summarize the loaded data\n",
    "    n_chars = len(text_data)\n",
    "    n_vocab = len(chars)\n",
    "    \n",
    "    return char_to_int, int_to_char, n_chars, n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch, starts, text_data, seq_length, batch_size, \n",
    "              char_to_int, n_vocab):\n",
    "    '''A generator that returns sequences of length seq_length, in\n",
    "       batches of size batch_size.\n",
    "       \n",
    "       Args:\n",
    "       - batch: (int) The index of the batch to be returned\n",
    "       - text_data: (str) The text to feed the model\n",
    "       - seq_length: (int) The length of each training sequence\n",
    "       - batch_size: (int) The size of minibatches for training'''\n",
    "    \n",
    "    # prepare the dataset of input to output pairs encoded as integers\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for start in range(batch_size * batch, batch_size * (batch + 1)): \n",
    "        seq_in  = text_data[starts[start]:starts[start] + seq_length]\n",
    "        seq_out = text_data[starts[start] + seq_length]\n",
    "        dataX.append([char_to_int[char] for char in seq_in])\n",
    "        dataY.append(char_to_int[seq_out])\n",
    "        \n",
    "    X = np_utils.to_categorical(dataX, num_classes=n_vocab)\n",
    "    X = X.reshape(batch_size, seq_length, n_vocab)\n",
    "\n",
    "    # one hot encode the output variable\n",
    "    y = np_utils.to_categorical(dataY, num_classes=n_vocab)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(mode, text_data, seq_length, validation_split,\n",
    "                     batch_size, char_to_int, n_chars, n_vocab,\n",
    "                     random_seed=42, shuffle=True):\n",
    "    '''A generator that returns training sequences of length seq_length, in\n",
    "       batches of size batch_size.\n",
    "\n",
    "       Args:\n",
    "       - mode: (str) Whether the batch is for training or validation. \n",
    "               'validation' or 'train' only\n",
    "       - text_data: (str) The text for training\n",
    "       - seq_length: (int) The length of each training sequence\n",
    "       - batch_size: (int) The size of minibatches for training\n",
    "       - validation_split: (float) The proportion of batches to use as \n",
    "                           validation data\n",
    "       - random_seed: A random seed'''\n",
    "\n",
    "    # set random seed\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # index the text_data\n",
    "    starts = list(range(n_chars - n_chars % seq_length - seq_length))\n",
    "    \n",
    "    if shuffle:\n",
    "        # shuffle the indices\n",
    "        random.shuffle(starts)\n",
    "    \n",
    "    while True:\n",
    "        n_batches = n_chars // batch_size\n",
    "        validation_size = round(n_batches * validation_split)\n",
    "        if mode == 'validation':\n",
    "            for batch in range(validation_size):\n",
    "                X, y = get_batch(batch, starts, text_data, seq_length, \n",
    "                                 batch_size, char_to_int, n_vocab)\n",
    "                yield X, y\n",
    "                \n",
    "        elif mode == 'train':\n",
    "            for batch in range(validation_size, n_batches):\n",
    "                X, y = get_batch(batch, starts, text_data, seq_length, \n",
    "                                 batch_size, char_to_int, n_vocab)\n",
    "                yield X, y\n",
    "        else:\n",
    "            raise ValueError(\"only 'validation' and 'train' modes accepted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(batch_size, seq_length, n_vocab, rnn_size, num_layers, \n",
    "                drop_prob, gpu_id):\n",
    "    '''Defines the RNN LSTM model.\n",
    "\n",
    "       Args:\n",
    "        - batch_size: (int) The size of each minibatches.\n",
    "        - seq_length: (int) The length of each sequence for the model.\n",
    "        - rnn_size: (int) The number of cells in each hidden layer.\n",
    "        - num_layers: (int) The number of hidden layers in the network.\n",
    "        - drop_prob: (float) The proportion of cells to drop in each dropout \n",
    "                             layer.\n",
    "       Returns:\n",
    "        - model: (keras.models.Sequential) The constructed Keras model.'''\n",
    "\n",
    "    model = Sequential()\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers - 1:\n",
    "            # add last hidden layer\n",
    "            model.add(LSTM(rnn_size, \n",
    "                           return_sequences=False,\n",
    "                           implementation=gpu_id))\n",
    "        elif i == 0:\n",
    "            # add first hidden layer - This crashes if num_layers == 1\n",
    "            model.add(LSTM(rnn_size, \n",
    "                           batch_input_shape=(None, seq_length, n_vocab),\n",
    "                           return_sequences=True,\n",
    "                           implementation=gpu_id))\n",
    "        else:\n",
    "            # add middle hidden layer\n",
    "            model.add(LSTM(rnn_size, \n",
    "                           return_sequences=True,\n",
    "                           implementation=gpu_id))\n",
    "        model.add(Dropout(drop_prob))\n",
    "    # add output layer\n",
    "    model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                  metrics=['accuracy'])  \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_callbacks(verbose, use_tensorboard, checkpoint_dir = \"checkpoints\"):\n",
    "    '''Set callbacks for Keras model.\n",
    "\n",
    "       Args:\n",
    "         - use_tensorboard: (int) Add TensorBoard callback if use_tensorboard == 1\n",
    "\n",
    "       Returns:\n",
    "         - callbacks: (list) list of callbacks for model'''        \n",
    "    root_dir = '.'\n",
    "    checkpoint_dir = os.path.join(root_dir,\n",
    "                                  checkpoint_dir, \n",
    "                                  'weights.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "    callbacks = [ModelCheckpoint(checkpoint_dir, verbose=verbose)]\n",
    "    if use_tensorboard:\n",
    "        log_dir = os.path.join('.', 'logs')\n",
    "        tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=0.01,\n",
    "                              write_images=True)\n",
    "        callbacks.append(tb_callback)  \n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, text_data, seq_length, validation_split, epochs, \n",
    "              batch_size, char_to_int, n_chars, n_vocab, verbose, use_tensorboard):\n",
    "    '''Trains the model on the training data.\n",
    "\n",
    "       Args:\n",
    "       - model:\n",
    "       - text_data:\n",
    "       - seq_length:\n",
    "       - batch_size:\n",
    "       - char_to_int:'''\n",
    "    n_batches = len(text_data) // batch_size\n",
    "    batch_params = (text_data, seq_length, validation_split,\n",
    "                     batch_size, char_to_int, n_chars, n_vocab)\n",
    "    hist = model.fit_generator(\n",
    "               generator = generate_batches('train', *batch_params),\n",
    "               validation_data = generate_batches('validation', *batch_params),\n",
    "               validation_steps = int(n_batches * validation_split),\n",
    "               workers = 1,\n",
    "               epochs = epochs,\n",
    "               steps_per_epoch = n_batches,\n",
    "               verbose = verbose,\n",
    "               callbacks = set_callbacks(verbose, use_tensorboard))\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory to the text file(s) for training.\n",
    "data_dir = \"./data\"\n",
    "# The checkpoint file for loading the model\n",
    "checkpoint = None\n",
    "# gpu id for GPU processing\n",
    "gpu_id = 1\n",
    "# The length of sequences to be used for training.\n",
    "seq_length = 25\n",
    "# The proportion of the training data to use for validation.\n",
    "validation_split = 0.1\n",
    "# The number of minibatches to be used for training.\n",
    "batch_size = 100\n",
    "# The number of cells in each hidden layer in the network. (int)\n",
    "rnn_size = 128\n",
    "# The number of hidden layers in the network\n",
    "num_layers = 3\n",
    "# Dropout value (between 0, 1 exclusive).\n",
    "drop_prob = 0.1\n",
    "# Number of epochs for training.\n",
    "epochs = 20\n",
    "# Set to 1 for verbose output, 0 to turn off.\n",
    "verbose = 1\n",
    "# Save model statistics to tensorboard.\n",
    "use_tensorboard = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data \n",
    "text_data = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int, int_to_char, n_chars, n_vocab = process_text(text_data, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpoint is not None:\n",
    "    # load model from checkpoint file\n",
    "    model = load_model(checkpoint)\n",
    "else:\n",
    "    # build and compile Keras model\n",
    "    model = build_model(batch_size, seq_length, n_vocab,\n",
    "                        rnn_size, num_layers, drop_prob, gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model using generator\n",
    "hist = fit_model(model, text_data, seq_length, validation_split, epochs,\n",
    "                 batch_size, char_to_int, n_chars, n_vocab,  \n",
    "                 verbose, use_tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
